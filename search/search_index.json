{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Session Overview","text":""},{"location":"#abstract","title":"Abstract","text":"<p>In today\u2019s fast-moving world of AI, large language models (for eg. GPT-4) are powerful tools - but they work best when guided with the right prompts. Isn\u2019t it frustrating when you straight-away ask an LLM \u201csummarise this article\u201d say, and get back something off the mark that makes it even more time-consuming? It\u2019s all in the communication \u2013 and that\u2019s exactly what we\u2019ll see. This session covers prompt engineering in detail and convinces how prompt design makes such a difference. It\u2019s nothing less than magic \ud83d\udd2e\ud83e\ude84 The same model can behave in entirely different ways, just by changing how you ask.</p> <p>We\u2019ll explore practical strategies for writing effective prompts. Through hands-on exercises, participants will practice building prompts for common use cases like question answering. The session is open to Foundational learners as well. We won\u2019t go deep into technical details, rather this talk is meant to be structured in a easy-to-understand way for beginners.</p> <p>Learning Outcomes \u2026</p> <p>By the end (you need to follow along!), you shall get a good essence of how to design better prompts, get more out of LLMs, and apply these skills to real-world scenarios.</p> <p>P.S. Undoubtedly, while a few sessions can never help one master something, this workshop is designed to give beginners and intermediate level learners the first spark and hopefully, the curiosity to keep exploring, experimenting &amp; learning more on their own.</p> <p>Activity</p> <p>An additional post-session mini activity will be released that participants can try, based on the concepts covered in the session - Check here to get started!</p>"},{"location":"#pre-requisites","title":"Pre-requisites?","text":"Please check before signing up! Target AudiencePrior knowledge checklist <p>Intended for beginner and intermediate level learners, interested in GenAI. If you are just starting out, it\u2019s fine. The session will cover from scratch.</p> <p>Estimated effort time will be ~ 120 to 180 mins.</p> <ul> <li>Basics of Python</li> <li>API key preferred for the hands-on segment (No prior set-up needed, we\u2019ll help create it during session).</li> </ul> <p>Suggested reading before the session: No such contents, reference papers will be shared later.</p>"},{"location":"#schedule","title":"Schedule?","text":"Agenda Date &amp; Time (IST) Google Meet Link Main talk, Hands-on Sat, October 18, 2025, 20.00 - 22.00 IST Join Google Meet Follow-up\u2020 QnA, hands-on cntd., task announcement, instructions explained Wed, October 29, 2025, 20.30 - 21.30 IST Join Google Meet <p>\u2020 There are ~600 registrants for this session. Even if half join, it won\u2019t be possible to address individual queries in one meet \u2014 those will be taken up on Day 2.</p> <p> For any questions regarding the sessions please send an email or post in the participants discussion space.</p> <p>Send </p> <p>\ud83d\udccc If you find this helpful, feel free to share. Explore, contribute, repeat.</p> <p>Icon credits: Flaticon</p>"},{"location":"ext/pg2/","title":"To be updated soon","text":""},{"location":"nb/test/","title":"Starter Notebook","text":"In\u00a0[1]: Copied! <pre>!pip install -q google-generativeai rouge-score tqdm scikit-learn\n\n# if running the code on local machine (say, VS Code)\n!pip install python-dotenv\n</pre> !pip install -q google-generativeai rouge-score tqdm scikit-learn  # if running the code on local machine (say, VS Code) !pip install python-dotenv <pre>Requirement already satisfied: python-dotenv in /home/somsubhra/Downloads/mkdocs/venv/lib/python3.12/site-packages (1.2.1)\r\n</pre> In\u00a0[2]: Copied! <pre>import pandas as pd\npd.set_option('display.max_colwidth', None)\n\n# Fetching the dataset, hosted online on the github page\nurl = \"https://mlig-iitmbs.github.io/somsubhra-promptingtalk/assets/squad-simplified.csv\"\ndf = pd.read_csv(url)\n\ndf.head()\n</pre> import pandas as pd pd.set_option('display.max_colwidth', None)  # Fetching the dataset, hosted online on the github page url = \"https://mlig-iitmbs.github.io/somsubhra-promptingtalk/assets/squad-simplified.csv\" df = pd.read_csv(url)  df.head() Out[2]: context question id text 0 Beyonc\u00e9 Giselle Knowles-Carter (/bi\u02d0\u02c8j\u0252nse\u026a/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&amp;B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyonc\u00e9's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\". When did Beyonce start becoming popular? 56be85543aeaaa14008c9063 in the late 1990s 1 Beyonc\u00e9 Giselle Knowles-Carter (/bi\u02d0\u02c8j\u0252nse\u026a/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&amp;B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyonc\u00e9's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\". What areas did Beyonce compete in when she was growing up? 56be85543aeaaa14008c9065 singing and dancing 2 Beyonc\u00e9 Giselle Knowles-Carter (/bi\u02d0\u02c8j\u0252nse\u026a/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&amp;B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyonc\u00e9's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\". When did Beyonce leave Destiny's Child and become a solo singer? 56be85543aeaaa14008c9066 2003 3 Beyonc\u00e9 Giselle Knowles-Carter (/bi\u02d0\u02c8j\u0252nse\u026a/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&amp;B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyonc\u00e9's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\". In what city and state did Beyonce  grow up? 56bf6b0f3aeaaa14008c9601 Houston, Texas 4 Beyonc\u00e9 Giselle Knowles-Carter (/bi\u02d0\u02c8j\u0252nse\u026a/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&amp;B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyonc\u00e9's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\". In which decade did Beyonce become famous? 56bf6b0f3aeaaa14008c9602 late 1990s In\u00a0[3]: Copied! <pre>df.shape\n</pre> df.shape Out[3]: <pre>(1000, 4)</pre> In\u00a0[4]: Copied! <pre>from sklearn.model_selection import train_test_split\n\ntrain_df, val_df = train_test_split(df, test_size=0.01, random_state=42)\n\nprint(f\"Training set: {train_df.shape}\")\nprint(f\"Validation set: {val_df.shape}\")\n</pre> from sklearn.model_selection import train_test_split  train_df, val_df = train_test_split(df, test_size=0.01, random_state=42)  print(f\"Training set: {train_df.shape}\") print(f\"Validation set: {val_df.shape}\") <pre>Training set: (990, 4)\nValidation set: (10, 4)\n</pre> In\u00a0[5]: Copied! <pre>import google.generativeai as genai\nimport random\nfrom rouge_score import rouge_scorer\nfrom tqdm import tqdm\n\n'''\n# If running on Colab, store your API key in Colab secrets &amp; provide nb access perms\n\nfrom google.colab import userdata\ngenai.configure(api_key=userdata.get('GEMINI_API_KEY'))\n'''\n\n# for local machine, please store the API key inside a `.env` file\nfrom dotenv import load_dotenv\nimport os\nload_dotenv()  # this loads the `.env` file\ngenai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\nmodel = genai.GenerativeModel(\"gemini-2.5-flash-lite\")\n</pre> import google.generativeai as genai import random from rouge_score import rouge_scorer from tqdm import tqdm  ''' # If running on Colab, store your API key in Colab secrets &amp; provide nb access perms  from google.colab import userdata genai.configure(api_key=userdata.get('GEMINI_API_KEY')) '''  # for local machine, please store the API key inside a `.env` file from dotenv import load_dotenv import os load_dotenv()  # this loads the `.env` file genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))  model = genai.GenerativeModel(\"gemini-2.5-flash-lite\") <pre>/home/somsubhra/Downloads/mkdocs/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[6]: Copied! <pre># We are picking 10 random samples from the `train` set.\nfew_shots = train_df.sample(10, random_state=42)\n\n# Here we are creating an instruction\ninstruction = (\n    \"You are a question answering model.\"\n    \"Given a context and a question, produce a concise and accurate answer.\"\n)\n\n# examples string\nexamples = \"\"\nfor _, row in few_shots.iterrows():\n    examples += (\n        f\"\\n\\nContext: {row['context']}\\n\"\n        f\"Question: {row['question']}\\n\"\n        f\"Answer: {row['text']}\"\n    )\n</pre> # We are picking 10 random samples from the `train` set. few_shots = train_df.sample(10, random_state=42)  # Here we are creating an instruction instruction = (     \"You are a question answering model.\"     \"Given a context and a question, produce a concise and accurate answer.\" )  # examples string examples = \"\" for _, row in few_shots.iterrows():     examples += (         f\"\\n\\nContext: {row['context']}\\n\"         f\"Question: {row['question']}\\n\"         f\"Answer: {row['text']}\"     ) In\u00a0[7]: Copied! <pre>predictions = []\nreferences = []\n\nfor _, row in tqdm(val_df.iterrows(), total=len(val_df)):\n    # main prompt\n    prompt = (\n        f\"{instruction}\\n\\nHere are some examples:{examples}\\n\\n\"\n        f\"Now answer this new question:\\n\\n\"\n        f\"Context: {row['context']}\\n\"\n        f\"Question: {row['question']}\\n\"\n        f\"Answer:\"\n    )\n\n    try:\n        response = model.generate_content(prompt)\n        pred = response.text.strip()\n    except Exception as e:\n        pred = \"\"\n\n    # appending the predicted outputs &amp; ground truths in respective lists\n    predictions.append(pred)\n    references.append(row[\"text\"])\n</pre> predictions = [] references = []  for _, row in tqdm(val_df.iterrows(), total=len(val_df)):     # main prompt     prompt = (         f\"{instruction}\\n\\nHere are some examples:{examples}\\n\\n\"         f\"Now answer this new question:\\n\\n\"         f\"Context: {row['context']}\\n\"         f\"Question: {row['question']}\\n\"         f\"Answer:\"     )      try:         response = model.generate_content(prompt)         pred = response.text.strip()     except Exception as e:         pred = \"\"      # appending the predicted outputs &amp; ground truths in respective lists     predictions.append(pred)     references.append(row[\"text\"])  <pre>\r  0%|                                                                                               | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761889569.288884    5068 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n</pre> <pre>\r 10%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                              | 1/10 [00:03&lt;00:35,  3.93s/it]</pre> <pre>\r 20%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                     | 2/10 [00:05&lt;00:20,  2.52s/it]</pre> <pre>\r 30%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                             | 3/10 [00:06&lt;00:13,  1.95s/it]</pre> <pre>\r 40%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                    | 4/10 [00:07&lt;00:09,  1.62s/it]</pre> <pre>\r 50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                           | 5/10 [00:09&lt;00:07,  1.54s/it]</pre> <pre>\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                  | 6/10 [00:10&lt;00:05,  1.47s/it]</pre> <pre>\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                          | 7/10 [00:11&lt;00:03,  1.31s/it]</pre> <pre>\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                 | 8/10 [00:12&lt;00:02,  1.33s/it]</pre> <pre>\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e        | 9/10 [00:14&lt;00:01,  1.35s/it]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:15&lt;00:00,  1.26s/it]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:15&lt;00:00,  1.54s/it]</pre> <pre>\n</pre> In\u00a0[8]: Copied! <pre>predictions\n</pre> predictions Out[8]: <pre>['2006',\n 'Hope for Haiti Now: A Global Benefit for Earthquake Relief telethon',\n 'God Bless the USA',\n '2010',\n 'hip hop',\n 'fall of 2015',\n 'Alison Krauss',\n 'Crazy in Love',\n 'Ludwika',\n 'Crazy in Love']</pre> In\u00a0[9]: Copied! <pre>references\n</pre> references Out[9]: <pre>['2006',\n 'Hope for Haiti Now: A Global Benefit',\n 'God Bless the USA',\n '2010.',\n 'hip hop',\n '2015',\n 'Alison Krauss',\n 'Crazy in Love',\n 'Ludwika',\n 'Crazy in Love']</pre> <p>ROUGE-N measures the number of matching n-grams between the model-generated text and reference.</p> <p>\u25b8 reference $R$ and the candidate summary $C$:</p> <p>$R$: The cat is on the mat.</p> <p>$C$: The cat and the dog.</p> <p>Say, for ROUGE-1:</p> <p>$P$: ratio of the number of unigrams in C that appear also in R, over the number of unigrams in C.</p> <p>$R$: ratio of the number of unigrams in R that appear also in C, over the number of unigrams in R.</p> <p>$$ F_1 = \\frac{2 \\cdot P \\cdot R}{P + R} $$</p> In\u00a0[10]: Copied! <pre># Let's check the quality of the generated outputs now, using an evaluation metric - ROUGE. It can range from [0,1]. The higher the score, the better is the model performance.\n\nscorer = rouge_scorer.RougeScorer([\"rouge1\"], use_stemmer=True)\n\nscores = {\"rouge1\": []}\nfor ref, pred in zip(references, predictions):\n    s = scorer.score(ref, pred)\n    scores[\"rouge1\"].append(s[\"rouge1\"].fmeasure)\n\nprint(f\"Average ROUGE-1: {sum(scores['rouge1'])/len(scores['rouge1']):.4f}\")\n</pre> # Let's check the quality of the generated outputs now, using an evaluation metric - ROUGE. It can range from [0,1]. The higher the score, the better is the model performance.  scorer = rouge_scorer.RougeScorer([\"rouge1\"], use_stemmer=True)  scores = {\"rouge1\": []} for ref, pred in zip(references, predictions):     s = scorer.score(ref, pred)     scores[\"rouge1\"].append(s[\"rouge1\"].fmeasure)  print(f\"Average ROUGE-1: {sum(scores['rouge1'])/len(scores['rouge1']):.4f}\")  <pre>Average ROUGE-1: 0.9278\n</pre>"},{"location":"nb/test/#this-is-a-starter-kit-that-serves-as-the-initial-setup-use-it-as-a-base-and-build-upon-it","title":"This is a starter kit that serves as the initial setup. Use it as a base and build upon it.\u00b6","text":""},{"location":"nb/test/#we-will-do-a-few-shot-prompting-with-10-random-samples-from-the-train-set-and-ask-the-llm-to-predict-on-the-test-set","title":"We will do a few shot prompting with $10$ random samples from the train set and ask the LLM) to predict on the test set.\u00b6","text":"<p>For more info on the model we're using, refer to the docs &amp; their rate limits!</p>"},{"location":"nb/test/#now-try-to-improve-the-score-by-refining-the-prompts-as-discussed-in-the-session-keep-experimenting","title":"Now, try to improve the score by refining the prompts, as discussed in the session. Keep experimenting. \ud83d\ude07\u00b6","text":""},{"location":"talk/feedback/","title":"Feedback","text":""},{"location":"talk/feedback/#_1","title":":)","text":"<p>Thoughts? Suggestions?</p> <p>I have tried to host it in a \u201cno-jargon\u201d approach. Please share your honest feedback at this link.</p> <p>If you found it exciting or insightful, what exactly did you like about the session? Did it really help you gain interest in the field of NLP, computational linguistics? What could\u2019ve been better? Please state your current knowledge level as well, so that I can know if the objectives for the session are somewhat fulfilled.</p> <p>If not satisfactory, please clearly state the reason- what didn\u2019t work for you? the concepts you couldn\u2019t connect to? Acc. to you, what lacked &amp; how can it be improved?</p>"},{"location":"talk/gradio/","title":"Gradio Sample NB","text":"<ul> <li> <p>Gradio is basically an open-source library that allows you to create a shareable interactive web interface from your Python code without the need of HTML/CSS/JS. You may go through this Medium blog - nicely explained!.</p> </li> <li> <p>Here we are creating a simple interactive 5-Round Palindrome Quiz game.</p> </li> </ul> <p>The application functions as follows:</p> <ol> <li>It presents a series of five random words and asks the user to classify each word as a Palindrome or Not a Palindrome using dedicated \u201cYes\u201d or \u201cNo\u201d buttons.</li> <li>Keeps track of the current round number and the user\u2019s running score.</li> <li>For each of the five rounds, it logs the following data- the Word presented, user\u2019s answer (Yes/No) and the Correct Answer (Yes/No).</li> <li>Whether the user was Correct (True/False).</li> <li>After all five rounds are complete, a \u201cSave Results to CSV\u201d button appears. Clicking this button compiles all the logged data into a pandas DataFrame and saves it as a file named <code>palindrome_quiz_results.csv</code> in the Colab nb directory.</li> </ol> <pre><code>!pip install gradio\n\nimport gradio as gr\nimport pandas as pd\nimport random\nimport os\n\nWORD_LIST = [\"level\", \"hello\", \"rotor\", \"world\", \"madam\", \"python\", \"coding\", \"refer\", \"data\"]\n\ndef is_palindrome(word):\n    return word.lower() == word.lower()[::-1]\n\ndef start_quiz():\n    initial_state = [\"\", 0, []]\n\n    next_word = random.choice(WORD_LIST)\n\n    initial_state[0] = next_word\n    initial_state[1] = 1\n\n    return (\n        initial_state,\n        f\"Question 1/5: Is '{next_word.upper()}' a palindrome?\",\n        gr.update(visible=True),\n        gr.update(visible=False)\n    )\n\ndef answer_question(answer, state):\n    current_word, current_round, results = state\n\n    correct_answer = \"Yes\" if is_palindrome(current_word) else \"No\"\n    is_correct = (answer == correct_answer)\n\n    results.append({\n        'Round': current_round,\n        'Word': current_word,\n        'User_Answer': answer,\n        'Correct_Answer': correct_answer,\n        'Correct': is_correct\n    })\n\n    next_round = current_round + 1\n\n    if next_round &gt; 5:\n        final_state = [\"\", next_round, results]\n        score = sum(r['Correct'] for r in results)\n\n        return (\n            final_state,\n            f\"Quiz Complete! Your Score: {score}/5\",\n            gr.update(visible=False),\n            gr.update(visible=True)\n        )\n    else:\n        next_word = random.choice(WORD_LIST)\n        new_state = [next_word, next_round, results]\n\n        return (\n            new_state,\n            f\"Question {next_round}/5: Is '{next_word.upper()}' a palindrome?\",\n            gr.update(visible=True),\n            gr.update(visible=False)\n        )\n\ndef save_results(state):\n    _, _, results = state\n    if not results:\n        return \"No results to save!\"\n\n    df = pd.DataFrame(results)\n    file_name = \"palindrome_quiz_results.csv\"\n\n    df.to_csv(file_name, index=False)\n\n    return f\"Results saved successfully to **{os.path.abspath(file_name)}** in your Colab environment.\"\n\n\nwith gr.Blocks(title=\"Palindrome Quiz\") as demo:\n    gr.Markdown(\"## \ud83d\udd21 Palindrome Quiz 5-Round Challenge\")\n\n    quiz_state = gr.State(value=[\"\", 0, []])\n\n    question_display = gr.Markdown(\"Click 'Start Quiz' to begin.\")\n    status_message = gr.Textbox(label=\"Status\", value=\"\", interactive=False)\n\n    with gr.Row():\n        start_btn = gr.Button(\"Start Quiz \ud83d\ude80\", variant=\"primary\")\n\n    with gr.Column(visible=False) as quiz_column:\n        with gr.Row():\n            yes_btn = gr.Button(\"Yes (Palindrome)\")\n            no_btn = gr.Button(\"No (Not Palindrome)\")\n\n    save_btn = gr.Button(\"\ud83d\udcbe Save Results to CSV\", visible=False, variant=\"secondary\")\n    save_output = gr.Textbox(label=\"Save Status\", interactive=False)\n\n\n    start_btn.click(\n        fn=start_quiz,\n        inputs=None,\n        outputs=[quiz_state, question_display, quiz_column, save_btn],\n        show_progress=False\n    )\n\n    yes_btn.click(\n        fn=answer_question,\n        inputs=[gr.State(value=\"Yes\"), quiz_state],\n        outputs=[quiz_state, question_display, quiz_column, save_btn],\n        show_progress=False\n    )\n\n    no_btn.click(\n        fn=answer_question,\n        inputs=[gr.State(value=\"No\"), quiz_state],\n        outputs=[quiz_state, question_display, quiz_column, save_btn],\n        show_progress=False\n    )\n\n    save_btn.click(\n        fn=save_results,\n        inputs=quiz_state,\n        outputs=save_output\n    )\n\ndemo.launch(inbrowser=True)\n</code></pre>"},{"location":"talk/materials/","title":"Session","text":""},{"location":"talk/materials/#links","title":"\ud83d\udd17 Links","text":"Resource Link Video recording - Meet-1 (18th Oct 2025) Watch Here <code>SQuAD</code> (simplified version) for basic hands-on Download Video recording - Meet-2 (29th Oct 2025) Watch Here"},{"location":"talk/topics/","title":"Topics Covered","text":""},{"location":"talk/topics/#its-all-in-the-prompt-the-secret-to-better-llm-outputs-key-discussion-pointers","title":"It\u2019s ALL in the Prompt!: The Secret to Better LLM Outputs - Key Discussion Pointers","text":"<ul> <li>Basics: What is a prompt? LLMs? How far have we come?</li> <li>Prompt engineering? Why is it so important? How has it changed the industrial &amp; NLP research landscape? some egs. from current research - prompt based learning</li> <li>0 programming, rather communicate well. An efficient way to engineer Human - model interaction? language is all you need!</li> <li>Vague input -&gt; vague responses, frustrating? How to effectively craft &amp; refine prompts and get desired outputs? How does it matter? Bias mitigation, hallucinations?</li> <li>Risks &amp; misuses, ethics, guardrails</li> <li>The art of prompting? same goal - different outputs? How diff prompting styles play a role - N shot, role-based, contextual, instructional, meta, CoT?</li> <li>MS POML- a new way to structure, maintain prompt templates</li> <li>Hands-on segment: Evaluation metrics in text summarization, Machine translation? How AI responds to prompts? Experimenting with Gemini 2.5 on QA tasks, SQuAD, How to optimize prompts &amp; achieve better scores?</li> </ul>"},{"location":"updates/","title":"Hands-on Activity!!","text":"<p>This is an individual mini-project. Participants must submit the deliverables by November 3rd.</p> Pre-requisites? <p>Please review all the provided resources before starting. Do not attempt this task without watching the video recordings \u2014 this activity serves as a checklist to test your ability to apply what we covered during the sessions. No additional materials are required; if you\u2019ve followed along with the sessions, you\u2019ll have everything you need to complete it.</p>"},{"location":"updates/#motivation-objective","title":"Motivation &amp; Objective","text":"<p>In traditional exams, teachers manually evaluate handwritten answer sheets - a time-consuming process often prone to human error. In this task, you will use Google\u2019s Gemini LLM to build an AI system that can automatically read, understand and evaluate student\u2019s handwritten answers from an image. To achieve a good performance, the LLM must handle challenges like inconsistent spelling, incomplete words, differences in handwriting styles, effectively distinguish actual answers from extra markings such as scribbles or underlines, which adds to the complexity.</p>"},{"location":"updates/#example","title":"Example","text":"<p> Let\u2019s look at this sample image. A answer sheet of a student consisting of SN (serial number), Statement (the questions) and the answer column consisting of the student\u2019s handwritten answer - True/False, for each of the \\(10\\) problems.</p> <p>You have the pre-defined answers and now you can compute the student\u2019s total score by matching each of the responses with the correct key. For eg., in this case, we have \\(3\\) matches (Questions \\(1\\), \\(4\\) and \\(7\\)), hence a score of \\(\\frac{3}{10}\\). Here\u2019s a sequence diagram version of the flow:</p> <pre><code>sequenceDiagram\n    participant User\n    participant AI_System\n    participant GeminiLLM\n    participant EvaluationLogic\n\n    User-&gt;&gt;AI_System: Submit answer sheet image + prompt\n    AI_System-&gt;&gt;GeminiLLM: Send image and evaluation prompt\n    GeminiLLM-&gt;&gt;AI_System: Return extracted answers (True/False)\n    AI_System-&gt;&gt;EvaluationLogic: Compare answers with key\n    EvaluationLogic-&gt;&gt;EvaluationLogic: Initialize score = 0, total = 10\n\n    loop For each question 1..10\n        EvaluationLogic-&gt;&gt;EvaluationLogic: Compare extracted answer[i] with key[i]\n        alt Correct answer\n            EvaluationLogic-&gt;&gt;EvaluationLogic: score = score + 1\n        else Incorrect answer\n            EvaluationLogic-&gt;&gt;EvaluationLogic: score unchanged\n        end\n    end\n\n    EvaluationLogic-&gt;&gt;AI_System: Return final score = score / total\n    AI_System-&gt;&gt;User: Display total score</code></pre>"},{"location":"updates/#milestones","title":"\ud83c\udfaf Milestones","text":"<ul> <li> <p>Create a Gradio web app running in Colab (Gradio provides user-friendly interface \u2014 no need for custom HTML, CSS or server setup\u2026 just a few lines of Python code and you\u2019re done!) that:</p> <ul> <li>accepts images (Download here \u2013&gt; sheet1, sheet2, sheet3, sheet4) of handwritten answer sheets in a fixed layout.</li> <li>uses <code>Gemini 2.5-flash</code>\u2019s (same model we used in the 1st session) multimodal capabilities to interpret the handwritten text.</li> <li>compares each extracted answer with the set of predefined correct answers.</li> <li>computes and displays the question number, student\u2019s transcribed response, correct key, the result (Correct/Incorrect) and the total score (refer to the demo interface images).</li> <li>(Optional: you can further make the flow dynamic by adding some more lines of Python code to save the scores from the previous sheet uploads, thus having a proper record.)</li> </ul> What does the interface look like? <p>Screenshots from a trial run.</p> Gradio launchedImage yet to be uploadedUploaded, LLM executes task, Scoring done <p></p> <p></p> <p></p> </li> </ul>"},{"location":"updates/#starter-code-for-gradio","title":"Starter code for Gradio","text":"gradio_demo.py<pre><code># Install Gradio first\n# !pip install gradio\n\nimport gradio as gr\nfrom PIL import Image\n\ndef evaluate_uploaded_image(image):\n    # TODO\n    # Placeholder for your processing logic\n    # Example --&gt; call Gemini LLM with prompt + image here\n    return \"Processed the uploaded image and computed score.\"\n\n# Sample Gradio interface\ndemo = gr.Interface(\n    fn=evaluate_uploaded_image,\n    inputs=gr.Image(type=\"pil\", label=\"Upload Handwritten Answer Sheet\"),\n    outputs=gr.Textbox(label=\"Evaluation Result\"),\n    title=\"Answer Sheet Evaluation\",\n    description=\"Upload a handwritten answer sheet image to get an automated score\",\n    allow_flagging=\"never\",\n)\n\n# Launch the app (works inside Colab or local runtime)\nif __name__ == \"__main__\":\n    demo.launch(share=True)\n</code></pre>"},{"location":"updates/#evaluation","title":"\ud83d\udcdd Evaluation","text":""},{"location":"updates/#pointers-for-better-performance","title":"Pointers for better performance?","text":"<ul> <li> <p>LLM\u2019s Accuracy: How reliably the model can read the student\u2019s handwriting (which can be messy, slanted, or oddly capitalized, check the images for ref.) - Depends on the prompt</p> </li> <li> <p>Scoring logic: ensures the transcribed answer is correctly mapped to the predefined correct key and the score is accurately computed.</p> </li> </ul>"},{"location":"updates/#important-notes","title":"Important Notes","text":"<p>Disclaimer: You are provided with only 4 samples<sup>1</sup> to test on, hence the nuance/actual complexity of such image processing tasks cannot be understood from this activity. This is meant to provide you a hands-on opportunity on a real-word problem.</p> <ul> <li> <p>Do NOT use any separate OCR tools (for eg, Tesseract). Instead, rely solely on prompt engineering techniques by crafting and refining prompts to help the AI model accurately extract and interpret handwritten answers from the provided images.</p> </li> <li> <p>The entire solution must be fully implemented within a single Google Colab notebook. Nothing fancy! (No need of separate webapp archs e.g., no Flask, Streamlit, or external backends etc.)</p> </li> <li> <p>You can use the same free tier API key that you had created initially. It should be more than enough, given the less number of samples &amp; the total call limits per day for this model. Goes without saying, accelerators/ GPUs not needed, utilise the standard CPU runtime of Colab.</p> </li> <li> <p>You are allowed to refer to AI tools (like ChatGPT, Perplexity, etc.) for solving this task, but please use them responsibly! Don\u2019t rely on them to generate the entire solution. Instead, take help when you\u2019re stuck and use them to guide you through each milestone step by step. Please declare the use of AI assistance in your submission (possibly in a <code>readme.md</code> file).</p> </li> </ul>"},{"location":"updates/#submission-guidelines","title":"\u2705 Submission Guidelines","text":"<ul> <li> <p> Execute the Colab notebook. Download (export) it in <code>.ipynb</code> format (NOT .py) with the cell outputs visible.</p> </li> <li> <p> Open the Gradio interface:</p> <ul> <li> <p> Upload each of the 4 test images.</p> </li> <li> <p> Take screenshots of the interface after each run (or after all uploads, depending on how you process the images - any modifications here are fine!).</p> </li> <li> <p> Ensure you capture contains: the transcribed answers for each question, comparison table (student\u2019s answer vs. correct key) and total score summary.</p> </li> </ul> </li> <li> <p> Finally, zip the <code>.ipynb</code>, images &amp; readme file - submit this single <code>rollnum.zip</code> file in the GForm.</p> </li> </ul> <p>Submit </p>"},{"location":"updates/#questions","title":"Questions?","text":"<p>Please post in the workshop discussion space or send an email.</p> <ol> <li> <p>Credits: This task has been provided, inspired by an IITJ Workshop on pattern recognition, at the NCVPRIPG\u201824 conference. The answer sheets used for setting up this activity are from their publicly available sample data. Thanks to the organizers!!\u00a0\u21a9</p> </li> </ol>"}]}