{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a starter kit that serves as the initial setup. Use it as a base and build upon it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T09:36:19.022157Z",
     "iopub.status.busy": "2025-10-15T09:36:19.021918Z",
     "iopub.status.idle": "2025-10-15T09:36:29.653677Z",
     "shell.execute_reply": "2025-10-15T09:36:29.652650Z",
     "shell.execute_reply.started": "2025-10-15T09:36:19.022138Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q google-generativeai rouge-score tqdm scikit-learn\n",
    "\n",
    "# if running the code on local machine (say, VS Code)\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:39:51.134984Z",
     "iopub.status.busy": "2025-10-15T13:39:51.134426Z",
     "iopub.status.idle": "2025-10-15T13:39:51.143179Z",
     "shell.execute_reply": "2025-10-15T13:39:51.142447Z",
     "shell.execute_reply.started": "2025-10-15T13:39:51.134962Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Fetching the dataset, hosted online on the github page\n",
    "url = \"https://mlig-iitmbs.github.io/somsubhra-promptingtalk/assets/squad-simplified.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.01, random_state=42)\n",
    "\n",
    "print(f\"Training set: {train_df.shape}\")\n",
    "print(f\"Validation set: {val_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will do a few shot prompting with $10$ random samples from the train set and ask the [LLM](https://en.wikipedia.org/wiki/Gemini_(language_model)) to predict on the test set.\n",
    "\n",
    "\n",
    "*For more info on the model we're using, refer to the [docs](https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-lite) & their [rate limits](https://ai.google.dev/gemini-api/docs/rate-limits)!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import random\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "\n",
    "'''\n",
    "# If running on Colab, store your API key in Colab secrets & provide nb access perms\n",
    "\n",
    "from google.colab import userdata\n",
    "genai.configure(api_key=userdata.get('GEMINI_API_KEY'))\n",
    "'''\n",
    "\n",
    "# for local machine, please store the API key inside a `.env` file\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()  # this loads the `.env` file\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash-lite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are picking 10 random samples from the `train` set.\n",
    "few_shots = train_df.sample(10, random_state=42)\n",
    "\n",
    "# Here we are creating an instruction\n",
    "instruction = (\n",
    "    \"You are a question answering model.\"\n",
    "    \"Given a context and a question, produce a concise and accurate answer.\"\n",
    ")\n",
    "\n",
    "# examples string\n",
    "examples = \"\"\n",
    "for _, row in few_shots.iterrows():\n",
    "    examples += (\n",
    "        f\"\\n\\nContext: {row['context']}\\n\"\n",
    "        f\"Question: {row['question']}\\n\"\n",
    "        f\"Answer: {row['text']}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for _, row in tqdm(val_df.iterrows(), total=len(val_df)):\n",
    "    # main prompt\n",
    "    prompt = (\n",
    "        f\"{instruction}\\n\\nHere are some examples:{examples}\\n\\n\"\n",
    "        f\"Now answer this new question:\\n\\n\"\n",
    "        f\"Context: {row['context']}\\n\"\n",
    "        f\"Question: {row['question']}\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        pred = response.text.strip()\n",
    "    except Exception as e:\n",
    "        pred = \"\"\n",
    "\n",
    "    # appending the predicted outputs & ground truths in respective lists\n",
    "    predictions.append(pred)\n",
    "    references.append(row[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUGE-N measures the number of matching n-grams between the model-generated text and reference.\n",
    "\n",
    "â–¸ reference $R$ and the candidate summary $C$:\n",
    "\n",
    "$R$: The cat is on the mat.\n",
    "\n",
    "$C$: The cat and the dog.\n",
    "\n",
    "Say, for ROUGE-1:\n",
    "\n",
    "$P$: ratio of the number of unigrams in C that appear also in R, over the number of unigrams in C.\n",
    "\n",
    "$R$: ratio of the number of unigrams in R that appear also in C, over the number of unigrams in R.\n",
    "\n",
    "$$\n",
    "F_1 = \\frac{2 \\cdot P \\cdot R}{P + R}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the quality of the generated outputs now, using an evaluation metric - ROUGE. It can range from [0,1]. The higher the score, the better is the model performance.\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\"], use_stemmer=True)\n",
    "\n",
    "scores = {\"rouge1\": []}\n",
    "for ref, pred in zip(references, predictions):\n",
    "    s = scorer.score(ref, pred)\n",
    "    scores[\"rouge1\"].append(s[\"rouge1\"].fmeasure)\n",
    "\n",
    "print(f\"Average ROUGE-1: {sum(scores['rouge1'])/len(scores['rouge1']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, try to improve the score by refining the prompts, as discussed in the session. Keep experimenting. ðŸ˜‡"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8498762,
     "sourceId": 13393192,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
